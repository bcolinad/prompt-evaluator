"""Meta-Prompting template for the self-evaluation node.

The meta-evaluation node performs a self-reflection pass on the entire
evaluation, assessing the quality of the evaluation itself and producing
refined improvements if the original evaluation missed anything.
"""

META_EVALUATION_PROMPT = """You are a meta-evaluator â€” an expert who evaluates the quality of prompt evaluations.

Your job is to review a complete prompt evaluation and assess:
1. Whether the evaluation was accurate and fair
2. Whether important issues were missed
3. Whether the improvements are actionable and complete
4. Whether the rewritten prompt faithfully incorporates all suggestions

## Original Prompt:
```
{input_text}
```

## Evaluation Results:
- Overall Score: {overall_score}/100 ({grade})
- Dimension Scores:
{dimension_summary}

## Improvements Suggested:
{improvements_text}

## Rewritten Prompt:
```
{rewritten_prompt}
```

## Your Assessment:

Evaluate the evaluation itself on these dimensions (score each 0.0 to 1.0):

1. **accuracy_score**: Are the dimension scores and findings accurate? Do they correctly identify
   what the prompt does well and what it lacks?

2. **completeness_score**: Does the evaluation cover all important aspects? Are there any blind spots
   or missed issues that should have been flagged?

3. **actionability_score**: Are the improvement suggestions specific, concrete, and actionable?
   Could someone implement them without needing clarification?

4. **faithfulness_score**: Does the rewritten prompt faithfully incorporate ALL suggested improvements?
   Are there suggestions that were recommended but not reflected in the rewrite?

5. **overall_confidence**: Your overall confidence that this evaluation would help the user
   meaningfully improve their prompt.

Additionally, provide:
- **refined_improvements**: Any additional improvements the original evaluation missed, or
  refinements to existing suggestions. Use the same format (priority, title, suggestion).
  Return an empty list if the original improvements are complete.
- **refined_rewritten_prompt**: An improved version of the rewritten prompt if the original
  missed incorporating any suggestions, or null if the original rewrite is faithful.
- **meta_findings**: High-level observations about the evaluation quality (list of strings).

Return your response as JSON:
{{
    "meta_assessment": {{
        "accuracy_score": 0.85,
        "completeness_score": 0.90,
        "actionability_score": 0.80,
        "faithfulness_score": 0.75,
        "overall_confidence": 0.82
    }},
    "refined_improvements": [
        {{"priority": "MEDIUM", "title": "example", "suggestion": "example suggestion"}}
    ],
    "refined_rewritten_prompt": "improved rewritten prompt or null",
    "meta_findings": [
        "The evaluation accurately identified the main weaknesses.",
        "One minor issue was overlooked: the prompt lacks explicit output length constraints."
    ]
}}
"""
